{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Homework 2: Implement, Tune and Evaluate your Policy Gradient and Actor-Critic Algorithms\n",
        "\n",
        "This assignment provides a hands-on introduction to the core components of Policy Gradient (PG) and Actor-Critic (AC) methods.\n",
        "\n",
        "For more details please checkout [HW2.md](../HW2.md)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "You will need to make a copy of this notebook in your Google Drive before you can edit the homework files. You can do so with **File &rarr; Save a copy in Drive**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtLnyycgIDf9"
      },
      "outputs": [],
      "source": [
        "#@title Mount Your Google Drive\n",
        "#@markdown Your work will be stored in a folder called `cs285_f2022` by default to prevent Colab instance timeouts from deleting your edits.\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9E_W9qzIDgA"
      },
      "outputs": [],
      "source": [
        "#@title Setup Mount Symlink\n",
        "\n",
        "DRIVE_PATH = '/content/gdrive/My\\ Drive/rl_class'\n",
        "DRIVE_PYTHON_PATH = DRIVE_PATH.replace('\\\\', '')\n",
        "if not os.path.exists(DRIVE_PYTHON_PATH):\n",
        "  %mkdir $DRIVE_PATH\n",
        "\n",
        "## the space in `My Drive` causes some issues,\n",
        "## make a symlink to avoid this\n",
        "SYM_PATH = '/content/rl_class'\n",
        "if not os.path.exists(SYM_PATH):\n",
        "  !ln -s $DRIVE_PATH $SYM_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Du7CaOkxIDgA"
      },
      "outputs": [],
      "source": [
        "#@title Apt Install Requirements\n",
        "\n",
        "#@markdown Run each section with Shift+Enter\n",
        "\n",
        "#@markdown Double-click on section headers to show code.\n",
        "\n",
        "!apt update\n",
        "!apt install -y --no-install-recommends \\\n",
        "        build-essential \\\n",
        "        curl \\\n",
        "        git \\\n",
        "        gnupg2 \\\n",
        "        make \\\n",
        "        cmake \\\n",
        "        ffmpeg \\\n",
        "        swig \\\n",
        "        libz-dev \\\n",
        "        unzip \\\n",
        "        zlib1g-dev \\\n",
        "        libglfw3 \\\n",
        "        libglfw3-dev \\\n",
        "        libxrandr2 \\\n",
        "        libxinerama-dev \\\n",
        "        libxi6 \\\n",
        "        libxcursor-dev \\\n",
        "        libgl1-mesa-dev \\\n",
        "        libgl1-mesa-glx \\\n",
        "        libglew-dev \\\n",
        "        libosmesa6-dev \\\n",
        "        lsb-release \\\n",
        "        ack-grep \\\n",
        "        patchelf \\\n",
        "        wget \\\n",
        "        xpra \\\n",
        "        xserver-xorg-dev \\\n",
        "        xvfb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2y-rZFSmIDgC"
      },
      "outputs": [],
      "source": [
        "#@title Clone Homework Repo\n",
        "\n",
        "%cd $SYM_PATH\n",
        "!git clone https://<TOKEN>@<Your Personalized Github Classroom Assignement link>\n",
        "# Use GitHub Personal Access Token as a password. Be careful not to expose your token; it's equivalent to a password!!! Don't commit your token!\n",
        "%cd <Your Personalized Github Classroom Assignement repo>\n",
        "%pip install swig\n",
        "%pip install -e .\n",
        "%pip install -r requirements_colab.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNmkPwbMKGFJ"
      },
      "outputs": [],
      "source": [
        "# # In case we need to install the dependencies manually\n",
        "# !pip install  tensorboard  torch swig gymnasium[box2d] ray[rllib] scikit-image pygame numba PyYAML GitPython tensorflow_probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Editing Code\n",
        "\n",
        "To edit code, click the folder icon on the left menu. Navigate to the corresponding file (`multigrid/...`). Double click a file to open an editor. There is a timeout of about ~12 hours with Colab while it is active (and less if you close your browser window). We sync your edits to Google Drive so that you won't lose your work in the event of an instance timeout, but you will need to re-mount your Google Drive and re-install packages with every new instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XACfqgP8IDgC"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from dataclasses import dataclass, asdict, field\n",
        "from types import SimpleNamespace\n",
        "import git\n",
        "from IPython.display import Image\n",
        "\n",
        "import ray\n",
        "from multigrid.envs import *\n",
        "from multigrid.utils.training_utilis import algorithm_config, get_checkpoint_dir\n",
        "from multigrid.scripts.train import configure_algorithm, train\n",
        "from multigrid.scripts.visualize import main_evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Fix Variables\n",
        "\n",
        "# Set the working diretory to the repo root\n",
        "REPO_ROOT = subprocess.check_output(['git', 'rev-parse', '--show-toplevel']).strip().decode('utf-8')\n",
        "os.chdir(REPO_ROOT)\n",
        "\n",
        "SUBMISSION_CONFIG_FILE = sorted(\n",
        "    Path(\"submission\").expanduser().glob(\"**/submission_config.json\"), key=os.path.getmtime\n",
        ")[-1]\n",
        "\n",
        "with open(SUBMISSION_CONFIG_FILE, \"r\") as file:\n",
        "    submission_config_data = file.read()\n",
        "\n",
        "submission_config = json.loads(submission_config_data)\n",
        "\n",
        "SUBMITTER_NAME = submission_config[\"name\"]\n",
        "\n",
        "CURRENT_DIR = os.getcwd()\n",
        "GIT_COMMIT_HASH =  git.Repo(REPO_ROOT).head.commit\n",
        "TAGS = {\"user_name\": SUBMITTER_NAME, \"git_commit_hash\": GIT_COMMIT_HASH}\n",
        "\n",
        "ALGORITHM_CONFIG_FILE = sorted(\n",
        "    Path(\"submission\").expanduser().glob(\"**/configs/algorithm_training_config.json\"), key=os.path.getmtime\n",
        ")[-1]\n",
        "\n",
        "with open(ALGORITHM_CONFIG_FILE, \"r\") as file:\n",
        "   algorithm_training_config_data = file.read()\n",
        "\n",
        "algorithm_training_config = json.loads(algorithm_training_config_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 - Familiarize Yourself with the ClearnRL PPO Implementation and Training Parameters for Deep RL Learning Loop\n",
        "First, check out the CleanRL PPO implementation and its configuration in [`multigrid/scripts/train_ppo_cleanrl.py`](multigrid/scripts/train_ppo_cleanrl.py). You can do this by running the following command with the `--debug-mode True` flag.\n",
        "\n",
        "Executing this command will display the default values of the training configuration and export a video showcasing the training scenario using random actions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python multigrid/scripts/train_ppo_cleanrl.py --debug-mode True "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1 Questions\n",
        "After running the above command, observe the outputs in the command line. This will provide essential information required to train your RL agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "#### Questions for General Deep RL Training Parameters Understanding\n",
        "**Q.1** From the command line outputs, can you report the values for the following parameters from the command line outputs? Additionally, please describe the role of each parameter in the training loop and explain how these values influence training in a sentence or two. This exercise can help you grasp the fundamentals of `Sample Efficiency` and understand the tradeoffs when scaling your training process in a parallel fashion.  \n",
        "\n",
        "- **num_envs**: \n",
        "- **batch_size**: \n",
        "- **num_minibatches**: \n",
        "- **minibatch_size**: \n",
        "- **total_timesteps**: \n",
        "- **num_updates**: \n",
        "- **num_steps**: \n",
        "- **update_epochs**: \n",
        "\n",
        "> **Note**: From Week 1, recall that `Sample Efficiency` refers to the ability of an algorithm to converge to an optimal solution with minimal sampling of experience data (trajectory from steps) from the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tips:\n",
        "- Refer to [Part 1 of 3 â€” Proximal Policy Optimization Implementation: 11 Core Implementation Details](https://www.youtube.com/watch?v=MEt6rrxH8W4) from Week 2's Curriculum.\n",
        "- Extensive comments and docstrings have been added atop the original [CleanRL ppo.py](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py) for your reference.\n",
        "- Explore different configurations for the V2 environment in `CONFIGURATIONS` within [envs/__init__.py].\n",
        "- Feel free to experiment with various arguments in [`multigrid/scripts/train_ppo_cleanrl.py`](multigrid/scripts/train_ppo_cleanrl.py) to familiarize yourself with this training script, its parameters, and the significance of the command line outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Notes:\n",
        "1. We only utilize the CleanRL PPO implementation in the first three main tasks of HW2. However, it offers a clean and straightforward way to grasp the ins and outs of the algorithm.\n",
        "2. It's beneficial to explore other PPO implementations in CleanRL's official repository. For example:\n",
        "    - [ppo_atari.py](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari.py)\n",
        "    - [ppo_atari_lstm.py](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_atari_lstm.py)\n",
        "    - [ppo_continuous_action.py](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo_continuous_action.py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "---\n",
        "## Task 2 - Understand the Deep RL Training Loop Dataflow & Implement Techniques to Minimize Learning Variance\n",
        "\n",
        "In this task, you will delve into the specifics of the vectorized training architecture, which consists of two pivotal phases: the `Rollout Phase` and the `Learning Phase`. This is the parallelized training architecture that many Deep RL algorithms, including PPO used. You will also explore the techniques employed by PPO to reduce variance in learning, particularly focusing on the Generalized Advantage Estimation (GAE). You will enhance your understanding by identifying these phases in the code and implementing GAE to reduce variance of the training data before the `Learning Phase` when using the diversed data collected from the `Rollout Phase`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Questions to Enhance Understanding of the Deep RL Training Loop\n",
        "***Q.1*** As mentioned in [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/), PPO employs a streamlined paradigm known as the vectorized architecture. This architecture encompasses two phases within the training loop:\n",
        "\n",
        "- **Rollout Phase**: During this phase, the agent samples actions for 'N' environments and continues to process them for a designated 'M' number of steps.\n",
        "\n",
        "- **Learning Phase**: In this phase, fundamentally, the agent learns from the data collected during the rollout phase. This data, with a length of NM, includes 'next_obs' and 'done'.\n",
        "\n",
        "Utilizing your baseline codebase tagged `v2.1`, please pinpoint the `Rollout Phase` and the `Learning Phase` within the codebase, indicating specific line numbers. \n",
        "\n",
        "* For instance, the lines [189-211 in CleanRL ppo.py](https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py#L189-L211) represent the Rollout Phase in their PPO implementation.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Q.2 How does PPO Reduce Variance? By Utilizing Generalized Advantage Estimation (GAE)? What is that?**\n",
        "\n",
        "> **Note**: \n",
        "  PPO employs the Generalized Advantage Estimation (GAE) method for advantage calculation, merging multiple n-step estimators into a singular estimate, thereby mitigating variance and fostering more stable and efficient training.\n",
        "\n",
        "  GAE amalgamates multiple n-step advantage estimators into a singular weighted estimator represented as:\n",
        "  \n",
        "      A_t^GAE(Î³,Î») = Î£(Î³Î»)^i Î´_(t+i)\n",
        "\n",
        "  \n",
        "  where:\n",
        "  \n",
        "      Î´_t - The temporal difference error formally defined as Î´_t = r_t + Î³V(s_(t+1)) - V(s_t)\n",
        "      Î³ - Discount factor which determines the weight of future rewards\n",
        "      Î» - A hyperparameter in [0,1] balancing bias and variance in the advantage estimation\n",
        "\n",
        "  **References**:\n",
        "  \"High-Dimensional Continuous Control Using Generalized Advantage Estimation\" by John Schulman et al.\n",
        "\n",
        "\n",
        "If you run the following training command to train an agent, you are expected to see ValueErrors from blanks that needed to be filled to implement and enable Generalized Advantage Estimation (GAE). Please make use of the comments in the code to help you to implement GAE. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Command for Task 2:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python multigrid/scripts/train_ppo_cleanrl.py --env-id MultiGrid-CompetativeRedBlueDoor-v2-DTDE-Red-Single --num-envs 8 --num-steps 128 --learning-rate 3e-4 --total-timesteps 10000000 --exp-name baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tips:\n",
        "- Useful comments has been added to the code for your guidance.\n",
        "- For further insight, you might refer to the [\"Generalized Advantage Estimation\" section in \"The 37 Implementation Details of Proximal Policy Optimization\"](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1R6jI4OT4Y9"
      },
      "source": [
        " ---\n",
        " ## Initialize and Show Tensorboard Before Training\n",
        "\n",
        " Filter tags for key performance metrics:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kurGcB2-RIgR"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66A3ck4ZRLTI"
      },
      "outputs": [],
      "source": [
        "# Start TensorBoard and Map the `logdir`` to `save_dir` i.e. `/content/gdrive/MyDrive/rl_class/week-1-intro-to-deep-rl-and-agent-training-environment/submission/cleanRL/runs`\n",
        "%tensorboard --logdir submission/cleanRL/runs --port 6007"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 3 - Tuning the ðŸŽ² **Exploration & Exploitation Strategies** using Algorithm-Specific Hyperparameters\n",
        "\n",
        "Having implemented GAE in Task 2, re-run the training command provided below to start agent training. You're encouraged to adjust or introduce additional parameters as required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! python ../multigrid/scripts/train_ppo_cleanrl.py --env-id MultiGrid-CompetativeRedBlueDoor-v2-DTDE-Red-Single-with-Obsticle --num-envs 8 --num-steps 128 --learning-rate 3e-4 --total-timesteps 10000000 --exp-name baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### Deepening Your Understanding to Interpret Your Results\n",
        "***Q.1*** Train a baseline agent using default or adjusted parameter values. Capture and present Tensorboard screenshots to report the following training metrics. Indicate the `Sample Effiicency`, the number of training timesteps and policy updates, required to achieve the Training Baseline Thresholds:\n",
        "\n",
        "- **episodic_length**\n",
        "- **episodic_return**\n",
        "- **policy_updates**\n",
        "- **entropy**\n",
        "- **explained_variance**\n",
        "- **value_loss**\n",
        "- **policy_loss**\n",
        "- **approx_kl**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**CleanRL Agent Training Baseline Thresholds for Your Reference**:\n",
        "- `episodic_length` should converge to a solution within 40 time steps and maintain for at least 100k time steps at the end of training.\n",
        "- `episodic_return` should converge to consistently achieve 2.0+ returns, enduring for a minimum of the last 100k time steps.\n",
        "- `explained_variance` should stabilize at a level above 0.6 for at least the last 100k time steps.\n",
        "- `entropy` should settle at values below 0.3 for a minimum of 100k final training steps.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hands-on Experiences on PPO-Specific Hyperparameters\n",
        "***Q.2*** If your baseline agent struggles to achieve the Training Baseline Thresholds, or if there's potential for enhancment, now you are getting the chance to fine-tuning the following PPO-specific parameters discussed in class to improve the performance of your agent. You may want to run multiple versions of experinements, so remember to modify `--exp-name` to differentiate between agent configurations. For final submissions, pick the top 3 performing or representable results and present the training metrics via screenshots and specify the number of timesteps and policy updates needed to fulfill or surpass the Training Baseline Thresholds. (Including links to their videos will be ideal)\n",
        "\n",
        "- **gamma**\n",
        "- **gae-lambda**\n",
        "- **clip-coef**\n",
        "- **clip-vloss**\n",
        "- **ent-coef**\n",
        "- **vf-coef**\n",
        "- **target-kl**\n",
        "\n",
        "Additionally, consider tweaking the following generic Deep RL hyperparameters:\n",
        "\n",
        "- **num_envs**\n",
        "- **batch_size**\n",
        "- **num_minibatches**\n",
        "- **minibatch_size**\n",
        "- **total_timesteps**\n",
        "- **num_updates**\n",
        "- **num_steps**\n",
        "- **update_epochs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tips:**\n",
        "- Monitor and track your runs using Tensorboard with the following command:\n",
        "  ```shell\n",
        "  tensorboard --logdir submission/cleanRL/runs\n",
        "  ```\n",
        "- Please refer Week2 lecture slide for the definition of the PPO-specific parameters\n",
        "- As mentioned in [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/): \n",
        "  - The significance of Value Function Loss Clipping is debatable. Engstrom, Ilyas, et al., (2020) didn't find evidence supporting its performance boost. In contrast, Andrychowicz, et al. (2021) inferred it might even hurt overall performance.\n",
        "  - For Early Stopping, consider setting the target kl to `0.01`, as demonstrated in [OpenAI's PPO PyTorch Implementation](https://spinningup.openai.com/en/latest/algorithms/ppo.html#documentation-pytorch-version). \n",
        "  - `CompetativeRedBlueDoorEnvV2` is redundent on purpose so that you can modify it with the Deep RL knowledge you have learned so far to solve the `MultiGrid-CompetativeRedBlueDoor-v2-DTDE-Red-Single-with-Obsticle` scenario. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Extra Challenge #1: Mastering a Scenario with Sparse Learning Signals and Lower Complexity\n",
        "\n",
        "Are you interested in challenging yourself further? Try to solve the scenario `MultiGrid-CompetitiveRedBlueDoor-v2-DTDE-Red-Single`. Intriguingly, the default agent configuration can solve the scenario with obstacle easily, but falters when there are none. \n",
        "\n",
        "\n",
        "See if you can solve the `MultiGrid-CompetativeRedBlueDoor-v2-DTDE-Red-Single`. Supprisingly, the default agent config can solve the scenario with obsticles, but not the scenario without. One possible explanation could be the close proximity of the obstacle to the door. Coupled with the annealing `ball_pickup_dense_reward`, these additional observations might act as additional learning signal, helping the agent overcome partial observability challenges and identify the sparse goal: opening the door using the key. But is this theory grounded in reality?\n",
        "\n",
        "We invite you to investigate this by addressing the same questions outlined in Task 3. Share your findings and discoveries from this challenge in your `HW2_Answer.md` file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tips**:\n",
        "- Be on the lookout for potential bugs lurking within the training code or the environment. Andy Jones' insightful piece, [Debugging RL, Without the Agonizing Pain](https://andyljones.com/posts/rl-debugging.html), can be a valuable resource in debugging the environment.\n",
        "- The `CompetitiveRedBlueDoorEnvV2` has been intentionally designed to be modifiable, and is not monitored by the CI/CD pipeline, providing you with the freedom to apply the deep RL concepts, you've learned so far to successfully navigate the `MultiGrid-CompetitiveRedBlueDoor-v2-DTDE-Red-Single` scenario.\n",
        "- Similarly, the `CompetitiveRedBlueDoorWrapperV2` was created to allow for customization, enabling you to alter the raw observations received from the unwrapped upstream environment. This grants you the flexibility to define your very own observation and action spaces."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Task 4: Bring the Lessons Learned from CleanRL to RLlib to solve a 1v1, ðŸ¤– ðŸ†š ðŸ¤– Scenario \n",
        "\n",
        "As you get familiar with PPO by working through the CleanRL implementation, let's pivot back to RLlib. We'll harness our understanding of hyperparameter tuning to address a 1v1 competition with a pre-trained opponent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## For Agent Training\n",
        "\n",
        "\n",
        "Once you are familiar with the new scenario `MultiGrid-CompetativeRedBlueDoor-v3-DTDE-1v1`, you can set up your Training Arguments below\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTZWvyvwYEGj"
      },
      "outputs": [],
      "source": [
        "#@title Training Arguments\n",
        "@dataclass\n",
        "class Args:\n",
        "\n",
        "    #@markdown agent config\n",
        "    num_agents: int = 1 #@param {type: \"integer\"}\n",
        "    algo: str = \"PPO\"  #@param {type: \"string\"}\n",
        "    framework: str = \"torch\" #@param ['torch', 'tf2']\n",
        "    lstm: bool = False #@param {type:\"boolean\"}\n",
        "    training_scheme: str = \"DTDE\"  #@param ['CTCE', 'DTDE', 'CTDE']\n",
        "    policies_to_train_raw: str = \"red_0\" #@param {type: \"raw\"}\n",
        "    policies_to_load_raw: str = \"blue_0\" #@param {type: \"raw\"}\n",
        "\n",
        "    #@markdown environemnt config\n",
        "    env: str = \"MultiGrid-CompetativeRedBlueDoor-v3-DTDE-1v1\"  #@param {type: \"string\"}\n",
        "\n",
        "    #@markdown training config\n",
        "    num_workers: int = 10  #@param {type: \"integer\"}\n",
        "    num_gpus: int = 0 #@param {type: \"integer\"}\n",
        "    lr: float = 0.001  #@param {type: \"float\"}\n",
        "    # NOTE Please only keep the checkpoints that you want to submit\n",
        "    save_dir: str = \"submission/ray_results/\" #@param {type: \"string\"}\n",
        "    load_dir: str = \"submission/pretrained_checkpoints/PPO_MultiGrid-CompetativeRedBlueDoor-v3-DTDE-1v1_154ab_00000_0_2023-09-12_16-08-06/checkpoint_000250\" #@param {type:\"string\"}\n",
        "    user_name: str = \"HengLee\" #@param {type: \"string\"}\n",
        "    experiment_name: str = \"1v1_Testing\" #@param {type: \"string\"}\n",
        "    mlflow_tracking_uri: str = \"submission/mlflow/\", #@param {type: \"string\"}\n",
        "    checkpoint_freq: int = 20 #@param {type: \"integer\"}\n",
        "    num_timesteps: float = 1e6 #@param {type: \"string\"}\n",
        "    checkpoint_freq: int = 20 #@param {type: \"integer\"}\n",
        "    seed: int = 1 #@param {type: \"integer\"}\n",
        "    local_mode: bool = False  #@param {type:\"boolean\"}\n",
        "\n",
        "    #@markdown Algorithm Specific Configs (Please enter valid JSON)\n",
        "    algorithm_training_config_str: str = '''{\n",
        "        \"algorithm_training_config\": {\n",
        "            \"PG_params\": {\n",
        "                \"gamma\": 0.99\n",
        "            },\n",
        "            \"PPO_params\": {\n",
        "                \"lambda_\": 0.99,\n",
        "                \"kl_coeff\": 0.2,\n",
        "                \"kl_target\": 0.01,\n",
        "                \"clip_param\": 0.3,\n",
        "                \"grad_clip\": null,\n",
        "                \"vf_clip_param\": 10.0,\n",
        "                \"vf_loss_coeff\": 0.5,\n",
        "                \"entropy_coeff\": 0.001,\n",
        "                \"sgd_minibatch_size\": 128,\n",
        "                \"num_sgd_iter\": 30\n",
        "            }\n",
        "        }\n",
        "    }''' #@param {type:\"raw\"}\n",
        "\n",
        "    algorithm_training_config: dict = field(default_factory=dict)\n",
        "\n",
        "    def to_namespace(self):\n",
        "        # Convert the JSON string to dictionary\n",
        "        parsed_algo_config = json.loads(self.algorithm_training_config_str)\n",
        "        self.algorithm_training_config = parsed_algo_config['algorithm_training_config']\n",
        "        return SimpleNamespace(**asdict(self))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJFHunlJPyfA"
      },
      "outputs": [],
      "source": [
        "#@title Set up Training Arguments\n",
        "args = Args().to_namespace()\n",
        "args.multiagent = {}\n",
        "args.multiagent[\"policies_to_train\"] = args.policies_to_train_raw.split(\",\")\n",
        "\n",
        "print(args) # Prints the values of all attributes\n",
        "\n",
        "config = configure_algorithm(args)\n",
        "stop_conditions = {\"timesteps_total\": args.num_timesteps}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XBVJyJQzLOcr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-09-16 15:18:29,751\tINFO tune.py:1111 -- Total run time: 134.28 seconds (123.96 seconds for the tuning loop).\n",
            "2023-09-16 15:18:29,753\tWARNING tune.py:1126 -- Experiment has been interrupted, but the most recent state was saved.\n",
            "Resume experiment with: tune.run(..., resume=True)\n"
          ]
        }
      ],
      "source": [
        "#@title Execute Training\n",
        "train(\n",
        "    algo=args.algo,\n",
        "    config=config,\n",
        "    stop_conditions=stop_conditions,\n",
        "    save_dir=args.save_dir,\n",
        "    load_dir=args.load_dir,\n",
        "    local_mode=args.local_mode,\n",
        "    experiment_name=args.experiment_name,\n",
        "    training_scheme=args.training_scheme,\n",
        "    policies_to_load=args.policies_to_load_raw.split(\",\"),  \n",
        ")\n",
        "\n",
        "# NOTE - Please remember to clear your training outputs before you submit your notebook to reduce file size and increase readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SB-Zsu5TQ2iN"
      },
      "outputs": [],
      "source": [
        "# NOTE-  Manually shutdown Ray if needed\n",
        "ray.shutdown()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q.1 Metrics to Report:\n",
        "\n",
        "As the same as Task 2&3, document the following training metrics, showcasing them with screenshots. Also, detail the number of timesteps and policy updates that meet or exceed the Training Baseline Thresholds.\n",
        "\n",
        "Here are the RLLib and Scenario specific metrics:\n",
        "\n",
        "- **episode_len_mean**\n",
        "- **ray/tune/policy_reward_mean/red_0**\n",
        "- **ray/tune/policy_reward_mean/blue_0**\n",
        "- **ray/tune/sampler_results/custom_metrics/red_0/door_open_done_mean**\n",
        "- **ray/tune/sampler_results/custom_metrics/blue_0/door_open_done_mean**\n",
        "- **ray/tune/sampler_results/custom_metrics/red_0/eliminated_opponents_done_mean**\n",
        "- **ray/tune/sampler_results/custom_metrics/blue_0/eliminated_opponents_done_mean**\n",
        "- **ray/tune/counters/num_agent_steps_trained**\n",
        "- **ray/tune/counters/num_agent_steps_sampled**\n",
        "- **ray/tune/counters/num_env_steps_sampled**\n",
        "- **ray/tune/counters/num_env_steps_trained**\n",
        "- **episodes_total**\n",
        "- **episodes_this_iter**\n",
        "- **red_0/learner_stats/cur_kl_coeff**\n",
        "- **red_0/learner_stats/entropy**\n",
        "- **red_0/learner_stats/grad_gnorm**\n",
        "- **red_0/learner_stats/kl**\n",
        "- **red_0/learner_stats/policy_loss**\n",
        "- **red_0/learner_stats/total_loss**\n",
        "- **red_0/learner_stats/vf_explained_var**\n",
        "- **red_0/learner_stats/vf_loss**\n",
        "- **red_0/num_grad_updates_lifetime**\n",
        "\n",
        "\n",
        "\n",
        "Here are the PPO-specific parameters in RLLib:\n",
        "- **gamma**\n",
        "- **lambda_**\n",
        "- **kl_coeff**\n",
        "- **kl_target**\n",
        "- **clip_param**\n",
        "- **grad_clip**\n",
        "- **vf_clip_param**\n",
        "- **vf_loss_coeff**\n",
        "- **entropy_coeff**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**RLlib Agent Training Baseline Thresholds for Your Reference**:\n",
        "- `episode_len_mean` should converge to a solution within 20 time steps and maintain for at least 100k time steps at the end of training.\n",
        "- `ray/tune/policy_reward_mean/red_0` should converge to consistently achieve 1.3+ returns, enduring for a minimum of the last 100k time steps.\n",
        "- `explained_variance` should stabilize at a level above 0.4 for at least the last 100k time steps.\n",
        "- `red_0/learner_stats/entropy` should settle at values below 0.3 for a minimum of 100k final training steps.\n",
        "\n",
        "**RLlib Agent Behavior Analysis Thresholds**\n",
        "The following Metrics are Behavior-specific metrics. It depends on how your agent emerges into certain specific behaviors to achieve the RL objective to maximize the discounted sum of rewards from time step t to the end of the game. So, how to achieve the maximum return depends on the training environment's world dynamic and the agent's reward structures. So, the \"Player Archetypes\" of your agent can be varied. \n",
        "\n",
        "Our training scenario can be interpreted as a Zero-Sum game. Therefore, if your agent learned to solve a particular scenario by unlocking the door first, your Red agent should dominate this metric. Vice Versa.\n",
        "- **ray/tune/sampler_results/custom_metrics/red_0/door_open_done_mean**\n",
        "- **ray/tune/sampler_results/custom_metrics/blue_0/door_open_done_mean**\n",
        "\n",
        "As mentioned above, if your agent learned to solve a particular scenario by eliminating the opponent first, your Red agent should dominate this metric. Vice Versa.\n",
        "- **ray/tune/sampler_results/custom_metrics/red_0/eliminated_opponents_done_mean**\n",
        "- **ray/tune/sampler_results/custom_metrics/blue_0/eliminated_opponents_done_mean**\n",
        "\n",
        "\n",
        "For final submitions, pick the top 3 performing or representable results and present the training metrics via screenshots and specify the number of timesteps and policy updates needed to fulfill or surpass the Training Baseline Thresholds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**Tips:**\n",
        "- Take a look at the configuration of `MultiGrid-CompetativeRedBlueDoor-v3-DTDE-1v1` in  [envs/__init__.py](multigrid/envs/__init__.py)\n",
        "- You can filter the plots using the following filters:\n",
        "\n",
        "```\n",
        "eliminated_opponents_done_mean|episode_len_mean|num_agent_steps_trained|num_agent_steps_sampled|num_env_steps_sampled|num_env_steps_trained|episodes_total|red_0/learner_stats/cur_kl_coeff|red_0/learner_stats/entropy|red_0/learner_stats/grad_gnorm|red_0/learner_stats/kl|red_0/learner_stats/policy_loss|red_0/learner_stats/total_loss|red_0/learner_stats/vf_explained_var|red_0/learner_stats/vf_loss|red_0/num_grad_updates_lifetime|ray/tune/policy_reward_mean/red_0|ray/tune/policy_reward_mean/blue_0\n",
        "```\n",
        "- RLlib Tune may report metrics with different names but pointing to the same metric. For example, `ray/tune/sampler_results/custom_metrics/blue_0/door_open_done_mean` is the same as `ray/tune/custom_metrics/blue_0/door_open_done_mean` so just report one is fine.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAdkjam5E2eW"
      },
      "source": [
        "## Submission for Task 4 - Monitor and Track Agent Training with Tensorboard and Save Out Visualization from Evaluation\n",
        "\n",
        "1. Please take screenshots of your Tensorboard plots that highlight your performance metrics\n",
        "2. Embedd your images here in CoLab\n",
        "3. Only save the best checkpoint and video in the /submission folder and push to your repo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8yqD1GYENlQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "from IPython.display import Image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Assuming a single image file is uploaded\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "  display(Image(fn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Your Tensorboard Screenshots Go Here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Extra Challenge #2: Exploring Different RLLib Algorithms\n",
        "Are you interested in challenging yourself further? See if you can solve the same scenario using alternative [Deep RL Algorithms](https://docs.ray.io/en/latest/rllib/rllib-algorithms.html#available-algorithms-overview) provided by RLLib.\n",
        "\n",
        "To experiment with different algorithms, adjust the `--algo` flag to specify an alternate RLlib-registered algorithm.\n",
        "\n",
        "**Tips**:\n",
        "- Review the supported conditions for each algorithm. Look for specific requirements or features like `Discrete Actions`, `Continuous Actions`, `Multi-Agent`, and `Model Support`.\n",
        "- Be aware: this RL class codebase is tested for PPO. Switching directly to other algorithms may introduce unexpected issues. The most straightforward adjustments might involve changing algorithm-specific parameters. More complex modifications could entail code changes to accommodate specific observation or action spaces.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Agent Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### *For running the Evaluation, we recommend restarting the kernel first to avoid any possible environmental issues that linger from training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBQbJDZSk_jK"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title Evaluation Arguments\n",
        "\n",
        "@dataclass\n",
        "class EvalArgs:\n",
        "\n",
        "  #@markdown agent config\n",
        "  algo: str = \"PPO\"  #@param {type: \"string\"}\n",
        "  framework: str = \"torch\" #@param ['torch', 'tf2']\n",
        "  lstm: bool = False #@param {type: \"boolean\"}\n",
        "\n",
        "  #@markdown environemnt config\n",
        "  env: str = \"MultiGrid-CompetativeRedBlueDoor-v3-DTDE-1v1\"  #@param {type: \"string\"}\n",
        "  env_config: Dict = field(default_factory=dict)  # Use default_factory to create a new dict for each instance\n",
        "\n",
        "  #@markdown Evaluation config\n",
        "  num_episodes: int = 10 #@param {type: \"integer\"}\n",
        "  load_dir: str = \"submission/ray_results/1v1_Testing/PPO_MultiGrid-CompetativeRedBlueDoor-v3-DTDE-1v1_9ec4e_00000_0_2023-09-15_22-42-04/\" #@param {type: \"string\"}\n",
        "  save_dir: str = \"submission/evaluation_reports/\" #@param {type: \"string\"}\n",
        "  gif: str = \"DTDE-1v1-testing\" #@param {type: \"string\"}\n",
        "  render_mode: str = \"human\" #@param {type: \"string\"}\n",
        "\n",
        "  def to_namespace(self):\n",
        "    return SimpleNamespace(**asdict(self))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eVxxRIIl4vm"
      },
      "outputs": [],
      "source": [
        "#@title Set up Evaluation Arguments\n",
        "\n",
        "eval_args = EvalArgs().to_namespace()\n",
        "print(eval_args)  # Prints the values of all attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8D9IPMyk-S1"
      },
      "outputs": [],
      "source": [
        "#@title Execute Evaluation\n",
        "exported_gif_filename = main_evaluation(args=eval_args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D9GMbtEaYQf"
      },
      "outputs": [],
      "source": [
        "#@title Visualize and Display Evaluated Agent Behavniors\n",
        "\n",
        "# Load the GIF\n",
        "display(Image(filename=exported_gif_filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recommendations for Homework Submission\n",
        "Backup and download your notebook first from Google CodeLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Makesure you are already in YOUR_REPO_FOLDER i.e. /content/rl_class/multigrid\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Configure Git\n",
        "!git config --global user.email \"your-email@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Move the Notebook to the Repo Folder (Optional)\n",
        "!mv \"your-homework1.ipynb\" \"/content/YOUR_REPO_FOLDER/notebooks\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Add, Commit and Push Changes to GitHub Classroom\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"Your commit message\"\n",
        "!git push origin main\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
